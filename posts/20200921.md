# Resourfce Management

## Medea

### Background

在共享（一些用户和应用程序共同使用）的生产集群中，机器学习，流传输和时延敏感的应用程序越来越普及，这给集群调度程序提出了新的挑战。 

原来常说那些作业，批处理作业，都是运行时间较短的容器，而这篇文章说的是那些长时间运行的容器。

而且这类程序一般都有些放置策略，比如有些容器确保放在一起，有些确保不要放在一起。

微软6个集群中，每个集群至少有10%用于LRA，有两个集群只跑 LRA。同时，将 LRA 和批处理放在一起很有吸引力，因为可以降低集群运营成本嘛，避免数据移动。

尽管如此，目前针对 LRA 的调度器都是非常基础的。容器放置的精确控制对于优化LRA的性能和复原能力至关重要。

### Design

- How to **relate containers** to node groups?
  - Support container tags and logical node groups
- How to **express** different types of constraints related to LRA containers?
  - Introduce expressive cardinality constraints
- How to achieve **high quality placement** without affecting task-based jobs?
  - Follow a two-scheduler design

### ILP-based scheduling algorithm

目标函数（等式1）具有三个组成部分：（1）放置尽可能多的k个LRA。 （2）最大限度地减了违反约束的次数； （3）通过最小化剩余资源少的节点数量来避免资源碎片。

为了线性地组合这些组件，而与它们的范围或单位无关，将每个组件归一化以采用0到1的值。

使用权重（w1-w3）为组件分配不同的优先级，具体值由管理员分配。

从这个ILP可以看出，他们侧重同时解决新容器的难题，而类似 Borg K8s 一次只考虑一个容器请求。

## Neptune

### Background

分布式数据处理框架是为了大数据处理。它经历一系列**变革**。

一开始分开的 batch 和 stream 框架，前者侧重高吞吐量，后者侧重低处理延迟。

这样的限制，意味着必须管理多个系统并在它们之间复制数据。

基于这样的局限性，统一的分布式数据流框架得到了发展。

朝着这个方向，最新的机器学习和 AI，这些系统又有了些统一的编程接口，以无缝表达流和批处理应用程序

关键词：大一统

用户开始把高吞吐量和低延迟的东西放在了一个应用里。

一个例子，针对恶意行为的实时检测服务。 批处理作业使用历史数据训练机器学习模型，而流作业执行推理以检测恶意行为。 

这种应用程序允许作业共享应用程序逻辑和状态（例如，在训练和推理之间使用相同的机器学习模型）。

这带来的主要挑战是：

* Latency: Execute inference job with minimum delay

- Throughput: Batch jobs should not be compromised
- Efficiency: Achieve high cluster resource utilization

### Stream/Batch application scheduling

EXEC-DIFF：基本方法：静态资源分配，每个 job 都使用专用的资源集。简单直观，但是缺点，时间很长，空白部分被完全浪费

FIFO：Spark and Flink 常用的方法。先来先得，batch 先来，吃掉所有资源，然后才是流处理，这导致流处理延迟很高

FAIR：有权重值，共享资源。这个例子下总时间的确很低，但是不尊重低延迟应用需求

KILL：保证了延迟低，代价显而易见，要重启被干掉的 batch 作业。那资源利用率其实很差。即使 job 支持保留进度的抢占，也必须考虑检查点 task 状态的额外开销。

### Neptune Design

* DAG 调度器计算每个 job 执行计划的 dag 图

* 构建执行顺序，相同任务分组并行计算，一个阶段全部完成以后下一阶段，stream 优先，剩余的 FIFO

- 任务到 task scheduler，然后具体执行，具有挂起策略，被挂起的不会被迁移，因为迁移会产生额外开销（论文解释不可以是因为流一般处理时间不长）
- Each executor has c cores and can run c concurrent tasks.
- 心跳，发送 executor 状态

### Suspendable Tasks

> **Neptune uses stackful coroutines to implement suspendable tasks, which have a yield point after the processing of each record.** 

* 对用户透明，不需要用户去维护一个变成编程模型，去触发一个阶段性的 checkpoint

* 通常来说挂起这件事我们会想到线程，一些原语比如说 wait 和 notify，线程的同步需要系统调用，这件事情是很消耗资源的，很快就会成为瓶颈。

* 协程是用户空间线程，为了获得可扩展且高效的任务挂起机制，Neptune 使用堆栈式协程。 协程作为可恢复的函数，可以在计算内部的 yield point 处暂停执行。常规函数被视为协程的特殊情况，协程不会中止执行，而是在完成后返回调用方。 当协程暂停执行时，它会将 status handle 返回给调用方，以后可将其用于恢复执行。

### LMA Scheduling policies

它干两件事

- 检查资源，2～5行，避免 executors 内存压力（内存使用，磁盘溢出，垃圾回收活跃度）
- 第二件事，调度，先检查有没有类似的任务，检查内存够不够，检查 CPU 够不够，都不行的话，抢占低有限度的

## Summary

* Medea：
  * 对于 Medea 而言，YARN Mesos 都提供了这种 LRA 的调度，而现在趋势显然就是 kubernetes（和它的前身 borg）。在这里面，只有 k8s 提供了一种标签的概念，但是没有 medea 来的那么精细，也不提供 cardinality。这个标签的概念可以给我们启发其实有好几个，首先，我们可不可以提供一些更为精细化的标签策略，比如说，关联等级，带有优先级的标签，比如说带有生命周期的标签。再比如，这种标签针对的是LRA，那显然也可以针对短期作业，还可以针对 VM。而这些策略如果我们把他放在 k8s 多租户集群，k8s 联邦集群，在这些场景下，它们势必带来更多的潜力。
  * 另外一点，k8s 它是一次考虑一个容器的调度，而我们可以看到 Medea 提供了一种 ILP 算法使得它一次性考虑多个容器。这种场景实际上还是和 short running container 有一些区分，可不可以更好的结合，可不可以尽可能减少批处理和流处理这些作业带给 LRA 的影响。或者说，在一些高资源占用的场景下，停掉一些不常用的 LRA 来换取更快的执行 batch/stream job。

* Neptune：这篇论文来说最大的启示就是协程下的暂停与恢复，它走了一条大家之前从未想过的道理，但是同样的，它也有一些空间，比如 stage 之间的资源空闲，学长的一篇论文就谈到了批作业的提前混洗和提前合并，我们是否能找到类似的手段提高这部分的资源空闲，比方说它目前的策略就是挂起和恢复，但是可能有些任务马上就要结束了，然后被挂起了很长时间，然后才恢复，那肯定也影响总体的性能，如果在资源允许情况下，这种任务的迁移所获得的价值肯定比简单的等要高效很多。